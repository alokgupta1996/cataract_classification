{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed085ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === 1. Load CLIP Model ===\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "\n",
    "def load_clip():\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    return model, preprocess\n",
    "\n",
    "# === 2. Pupil Cropping ===\n",
    "def crop_to_pupil(image_path, output_size=(784, 784)):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blur = cv2.medianBlur(gray, 5)\n",
    "\n",
    "    circles = cv2.HoughCircles(gray_blur, cv2.HOUGH_GRADIENT, dp=1.5, minDist=30,\n",
    "                                param1=50, param2=30, minRadius=20, maxRadius=150)\n",
    "\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        x, y, r = circles[0][0]\n",
    "        pad = int(r * 1.5)\n",
    "        x1, y1 = max(0, x - pad), max(0, y - pad)\n",
    "        x2, y2 = min(image.shape[1], x + pad), min(image.shape[0], y + pad)\n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "    else:\n",
    "        print(f\"⚠️ Pupil not detected in {image_path}, using full image.\")\n",
    "        cropped = image\n",
    "\n",
    "    resized = cv2.resize(cropped, output_size)\n",
    "    return resized\n",
    "\n",
    "# === 3. Save Cropped Images ===\n",
    "def preprocess_folder(input_folder, output_folder, size=(784, 784)):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in tqdm(os.listdir(input_folder)):\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            path = os.path.join(input_folder, fname)\n",
    "            cropped = crop_to_pupil(path, output_size=size)\n",
    "            save_path = os.path.join(output_folder, fname)\n",
    "            cv2.imwrite(save_path, cropped)\n",
    "\n",
    "# === 4. Augmentation (optional) ===\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2)\n",
    "])\n",
    "\n",
    "def augment_image(img_pil, n_augmentations=2):\n",
    "    augmented = [augmentation_transforms(img_pil) for _ in range(n_augmentations)]\n",
    "\n",
    "    # Grayscale version\n",
    "    grayscale = transforms.Grayscale()(img_pil)\n",
    "    augmented.append(grayscale.convert(\"RGB\"))  # Ensure 3 channels\n",
    "\n",
    "    # Edge-detected (Canny via OpenCV)\n",
    "    np_img = np.array(img_pil)\n",
    "    gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    edge_pil = Image.fromarray(edge_rgb)\n",
    "    augmented.append(edge_pil)\n",
    "\n",
    "    return augmented\n",
    "\n",
    "# === 5. Embedding Extraction ===\n",
    "def get_embeddings(image_path, preprocess, augment=False, n_aug=3):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    images = [image]\n",
    "    if augment:\n",
    "        images += augment_image(image, n_augmentations=n_aug)\n",
    "\n",
    "    embeddings = []\n",
    "    for img in images:\n",
    "        tensor = preprocess(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            feat = model.encode_image(tensor)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        embeddings.append(feat.numpy())\n",
    "\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# === 6. Build DataFrame from Folder ===\n",
    "def build_dataframe(folder, label, preprocess, augment=True, n_aug=3):\n",
    "    df = pd.DataFrame(columns=range(512))\n",
    "    idx = 0\n",
    "    for image_path in tqdm(glob(f\"{folder}/*.png\")):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # Original\n",
    "        tensor = preprocess(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            feat = model.encode_image(tensor)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "#         print(feat.numpy(),len(feat.numpy()[0]))\n",
    "        df.loc[idx, list(range(512))] = list(feat.numpy()[0])\n",
    "        df.loc[idx, 'category'] = label\n",
    "        idx += 1\n",
    "\n",
    "        # Augmented\n",
    "        if augment:\n",
    "            augmented_imgs = augment_image(image, n_augmentations=n_aug)\n",
    "            for img in augmented_imgs:\n",
    "                tensor = preprocess(img).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    feat = model.encode_image(tensor)\n",
    "                    feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "                df.loc[idx, list(range(512))] = list(feat.numpy()[0])\n",
    "                df.loc[idx, 'category'] = label\n",
    "                idx += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# === 7. Train Model ===\n",
    "def train_classifier(X, y):\n",
    "    model = LGBMClassifier(random_state=42,**{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200})\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# === 8. Evaluate Model ===\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2644935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = load_clip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "727841df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 246/246 [00:05<00:00, 4\n",
      "100%|█| 245/245 [00:05<00:00, 4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Run once to preprocess images\n",
    "preprocess_folder(\"processed_eye_images//train/normal\", \"processed_aug/train/normal\")\n",
    "preprocess_folder(\"processed_eye_images/train/cataract\", \"processed_aug/train/cataract\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afe1780b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.008209</td>\n",
       "      <td>-0.025388</td>\n",
       "      <td>0.170292</td>\n",
       "      <td>-0.076664</td>\n",
       "      <td>-0.011962</td>\n",
       "      <td>-0.046452</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>0.052535</td>\n",
       "      <td>-0.040558</td>\n",
       "      <td>0.072361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>-0.014476</td>\n",
       "      <td>-0.018878</td>\n",
       "      <td>-0.042237</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.021294</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.018250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029179</td>\n",
       "      <td>0.133555</td>\n",
       "      <td>-0.067828</td>\n",
       "      <td>-0.077476</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>-0.029483</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>-0.019857</td>\n",
       "      <td>-0.033151</td>\n",
       "      <td>0.054449</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007106</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>-0.036768</td>\n",
       "      <td>0.027888</td>\n",
       "      <td>0.012523</td>\n",
       "      <td>0.011096</td>\n",
       "      <td>-0.014833</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033629</td>\n",
       "      <td>0.070846</td>\n",
       "      <td>0.069969</td>\n",
       "      <td>-0.076290</td>\n",
       "      <td>-0.015656</td>\n",
       "      <td>-0.042773</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>-0.020503</td>\n",
       "      <td>0.038118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.026400</td>\n",
       "      <td>-0.025397</td>\n",
       "      <td>-0.030248</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>-0.009309</td>\n",
       "      <td>-0.024477</td>\n",
       "      <td>-0.004683</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020165</td>\n",
       "      <td>-0.006758</td>\n",
       "      <td>-0.010106</td>\n",
       "      <td>-0.073266</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>-0.055530</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>-0.010074</td>\n",
       "      <td>-0.026962</td>\n",
       "      <td>0.074223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>-0.046412</td>\n",
       "      <td>-0.042317</td>\n",
       "      <td>0.044431</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>-0.014756</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008486</td>\n",
       "      <td>0.018246</td>\n",
       "      <td>-0.070254</td>\n",
       "      <td>-0.082442</td>\n",
       "      <td>-0.006956</td>\n",
       "      <td>-0.037441</td>\n",
       "      <td>0.034377</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>-0.003671</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>0.008325</td>\n",
       "      <td>0.036520</td>\n",
       "      <td>-0.056718</td>\n",
       "      <td>-0.012834</td>\n",
       "      <td>0.018885</td>\n",
       "      <td>-0.021029</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>-0.005907</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.019468</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.084389</td>\n",
       "      <td>-0.089938</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>-0.014791</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>-0.027107</td>\n",
       "      <td>0.044697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078755</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>-0.032743</td>\n",
       "      <td>-0.016124</td>\n",
       "      <td>-0.041180</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>-0.004320</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>0.012522</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.045920</td>\n",
       "      <td>0.040774</td>\n",
       "      <td>0.080929</td>\n",
       "      <td>-0.087540</td>\n",
       "      <td>-0.007057</td>\n",
       "      <td>-0.006279</td>\n",
       "      <td>0.027199</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>-0.017314</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066825</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>-0.030704</td>\n",
       "      <td>-0.024800</td>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>-0.015694</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.043656</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.009453</td>\n",
       "      <td>-0.051162</td>\n",
       "      <td>0.103528</td>\n",
       "      <td>-0.092911</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>-0.039286</td>\n",
       "      <td>0.013243</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>-0.024395</td>\n",
       "      <td>0.046551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059349</td>\n",
       "      <td>0.014583</td>\n",
       "      <td>-0.007841</td>\n",
       "      <td>-0.051091</td>\n",
       "      <td>-0.025609</td>\n",
       "      <td>0.025573</td>\n",
       "      <td>-0.019856</td>\n",
       "      <td>0.025146</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.124875</td>\n",
       "      <td>-0.043288</td>\n",
       "      <td>-0.019162</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>-0.058726</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>-0.003096</td>\n",
       "      <td>-0.040012</td>\n",
       "      <td>0.063139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025831</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>-0.029976</td>\n",
       "      <td>-0.039640</td>\n",
       "      <td>-0.006650</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>-0.017334</td>\n",
       "      <td>-0.002568</td>\n",
       "      <td>0.037007</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>-0.022084</td>\n",
       "      <td>-0.074055</td>\n",
       "      <td>-0.078704</td>\n",
       "      <td>-0.054631</td>\n",
       "      <td>0.035355</td>\n",
       "      <td>-0.027510</td>\n",
       "      <td>0.012538</td>\n",
       "      <td>-0.013447</td>\n",
       "      <td>-0.033968</td>\n",
       "      <td>0.076915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>-0.041939</td>\n",
       "      <td>-0.018812</td>\n",
       "      <td>0.027819</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.016136</td>\n",
       "      <td>0.038225</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.008209 -0.025388  0.170292 -0.076664 -0.011962 -0.046452  0.011933   \n",
       "1    0.029179  0.133555 -0.067828 -0.077476  0.001874 -0.029483  0.039485   \n",
       "2    0.033629  0.070846  0.069969 -0.076290 -0.015656 -0.042773  0.014903   \n",
       "3    0.020165 -0.006758 -0.010106 -0.073266  0.015090 -0.055530  0.015667   \n",
       "4    0.008486  0.018246 -0.070254 -0.082442 -0.006956 -0.037441  0.034377   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "486  0.019468  0.006743  0.084389 -0.089938 -0.001608 -0.014791  0.010211   \n",
       "487  0.045920  0.040774  0.080929 -0.087540 -0.007057 -0.006279  0.027199   \n",
       "488  0.009453 -0.051162  0.103528 -0.092911  0.020825 -0.039286  0.013243   \n",
       "489  0.020057  0.124875 -0.043288 -0.019162  0.034511 -0.058726  0.000202   \n",
       "490 -0.022084 -0.074055 -0.078704 -0.054631  0.035355 -0.027510  0.012538   \n",
       "\n",
       "            7         8         9  ...       503       504       505  \\\n",
       "0    0.052535 -0.040558  0.072361  ...  0.015106 -0.014476 -0.018878   \n",
       "1   -0.019857 -0.033151  0.054449  ... -0.007106  0.022177  0.007312   \n",
       "2    0.011056 -0.020503  0.038118  ...  0.039820 -0.005678 -0.026400   \n",
       "3   -0.010074 -0.026962  0.074223  ...  0.014216 -0.000033  0.028111   \n",
       "4    0.007716 -0.003671  0.010978  ...  0.014514  0.008325  0.036520   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "486  0.022423 -0.027107  0.044697  ...  0.078755  0.005462 -0.032743   \n",
       "487  0.011280 -0.017314  0.064901  ...  0.066825  0.015700 -0.030704   \n",
       "488  0.008333 -0.024395  0.046551  ...  0.059349  0.014583 -0.007841   \n",
       "489 -0.003096 -0.040012  0.063139  ...  0.025831  0.002300 -0.029976   \n",
       "490 -0.013447 -0.033968  0.076915  ...  0.019454  0.000378  0.008578   \n",
       "\n",
       "          506       507       508       509       510       511  category  \n",
       "0   -0.042237  0.009638  0.021294  0.028043 -0.001386 -0.018250       0.0  \n",
       "1   -0.036768  0.027888  0.012523  0.011096 -0.014833  0.007427       0.0  \n",
       "2   -0.025397 -0.030248  0.015482 -0.009309 -0.024477 -0.004683       0.0  \n",
       "3   -0.046412 -0.042317  0.044431  0.005613  0.015509 -0.014756       1.0  \n",
       "4   -0.056718 -0.012834  0.018885 -0.021029  0.011422 -0.005907       1.0  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "486 -0.016124 -0.041180  0.010931 -0.004320  0.004352  0.012522       0.0  \n",
       "487 -0.024800 -0.052801  0.002484 -0.015694  0.006363  0.043656       0.0  \n",
       "488 -0.051091 -0.025609  0.025573 -0.019856  0.025146  0.012634       1.0  \n",
       "489 -0.039640 -0.006650  0.007998 -0.017334 -0.002568  0.037007       1.0  \n",
       "490 -0.041939 -0.018812  0.027819  0.007072  0.016136  0.038225       1.0  \n",
       "\n",
       "[491 rows x 513 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7236f317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 246/246 [00:58<00:00,  \n",
      "100%|█| 245/245 [00:58<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1176, number of negative: 1180\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 2356, number of used features: 512\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499151 -> initscore=-0.003396\n",
      "[LightGBM] [Info] Start training from score -0.003396\n",
      "[[280  16]\n",
      " [ 19 275]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.95      0.94       296\n",
      "         1.0       0.95      0.94      0.94       294\n",
      "\n",
      "    accuracy                           0.94       590\n",
      "   macro avg       0.94      0.94      0.94       590\n",
      "weighted avg       0.94      0.94      0.94       590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_normal = build_dataframe(\"processed_aug/train/normal\", 0, preprocess)\n",
    "df_cataract = build_dataframe(\"processed_aug/train/cataract\", 1, preprocess)\n",
    "df = pd.concat([df_normal, df_cataract]).astype(float).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df.iloc[:, :512]\n",
    "y = df['category']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a342663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[278  18]\n",
      " [ 17 277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94       296\n",
      "         1.0       0.94      0.94      0.94       294\n",
      "\n",
      "    accuracy                           0.94       590\n",
      "   macro avg       0.94      0.94      0.94       590\n",
      "weighted avg       0.94      0.94      0.94       590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "clf = train_classifier(X_train, y_train)\n",
    "evaluate_model(clf, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9997c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2946, 512)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77103d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: LGBMClassifier(max_depth=7, n_estimators=200, verbose=-1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LGBMClassifier(verbose=-1), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best model:\", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4b5c8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4082dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def crop_to_pupil(image_path, output_size=(784, 784)):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blur = cv2.medianBlur(gray, 5)\n",
    "\n",
    "    circles = cv2.HoughCircles(gray_blur, cv2.HOUGH_GRADIENT, dp=1.5, minDist=30,\n",
    "                                param1=50, param2=30, minRadius=20, maxRadius=150)\n",
    "\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        x, y, r = circles[0][0]\n",
    "        pad = int(r * 1.5)\n",
    "        x1, y1 = max(0, x - pad), max(0, y - pad)\n",
    "        x2, y2 = min(image.shape[1], x + pad), min(image.shape[0], y + pad)\n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "    else:\n",
    "        print(f\"⚠️ Pupil not detected in {image_path}, using full image.\")\n",
    "        cropped = image\n",
    "\n",
    "    resized = cv2.resize(cropped, output_size)\n",
    "    return resized\n",
    "\n",
    "def infer_on_folder(folder_path, clf, preprocess, model):\n",
    "    results = []\n",
    "    image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for image_path in tqdm(image_paths, desc=\"🔍 Running Inference\"):\n",
    "        cropped = crop_to_pupil(image_path)\n",
    "        image_pil = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "        image_tensor = preprocess(image_pil).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = model.encode_image(image_tensor)\n",
    "            embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        pred = clf.predict(embedding.numpy())[0]\n",
    "        prob = clf.predict_proba(embedding.numpy())[0]\n",
    "\n",
    "        results.append({\n",
    "            'image_path': image_path,\n",
    "            'prediction': 'cataract' if pred == 1 else 'normal',\n",
    "            'prob_cataract': round(prob[1], 4),\n",
    "            'prob_normal': round(prob[0], 4),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1a07a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Running Inference:  62%|▌| 3libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Running Inference: 100%|█| 6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     image_path prediction  prob_cataract  \\\n",
      "0  processed_images/test/cataract/image_279.png   cataract         1.0000   \n",
      "1  processed_images/test/cataract/image_251.png   cataract         0.9936   \n",
      "2  processed_images/test/cataract/image_292.png   cataract         1.0000   \n",
      "3  processed_images/test/cataract/image_286.png   cataract         1.0000   \n",
      "4  processed_images/test/cataract/image_287.png   cataract         0.9999   \n",
      "\n",
      "   prob_normal  \n",
      "0       0.0000  \n",
      "1       0.0064  \n",
      "2       0.0000  \n",
      "3       0.0000  \n",
      "4       0.0001  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = load_clip()\n",
    "\n",
    "# Run inference\n",
    "df_results = infer_on_folder(\"processed_images/test/cataract/\", clf, preprocess, model)\n",
    "\n",
    "# Show results\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da395a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['cat'] = df_results['prob_cataract']>0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "163fc41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results['cat'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "885f34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Running Inference:  25%|▎| 1libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Running Inference:  43%|▍| 2libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Running Inference:  62%|▌| 3libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Running Inference:  72%|▋| 4libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Running Inference:  90%|▉| 5libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Running Inference: 100%|█| 6\n"
     ]
    }
   ],
   "source": [
    "df_results = infer_on_folder(\"processed_images/test/normal/\", clf, preprocess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa8e7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['cat'] = df_results['prob_normal']>0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73eb8321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results['cat'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db42446",
   "metadata": {},
   "source": [
    "## Check effect of augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "313c1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# === 1. Load CLIP Model ===\n",
    "def load_clip(device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "    model.to(device).eval()\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    return model, preprocess, tokenizer, device\n",
    "\n",
    "# === 2. Pupil Cropping ===\n",
    "def crop_to_pupil(image_path, output_size=(784, 784)):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blur = cv2.medianBlur(gray, 5)\n",
    "\n",
    "    circles = cv2.HoughCircles(gray_blur, cv2.HOUGH_GRADIENT, dp=1.5, minDist=30,\n",
    "                                param1=50, param2=30, minRadius=20, maxRadius=150)\n",
    "\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        x, y, r = circles[0][0]\n",
    "        pad = int(r * 1.5)\n",
    "        x1, y1 = max(0, x - pad), max(0, y - pad)\n",
    "        x2, y2 = min(image.shape[1], x + pad), min(image.shape[0], y + pad)\n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "    else:\n",
    "        print(f\"⚠️ Pupil not detected in {image_path}, using full image.\")\n",
    "        cropped = image\n",
    "\n",
    "    resized = cv2.resize(cropped, output_size)\n",
    "    return resized\n",
    "\n",
    "# === 3. Save Cropped Images ===\n",
    "def preprocess_folder(input_folder, output_folder, size=(784, 784)):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in tqdm(os.listdir(input_folder)):\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            path = os.path.join(input_folder, fname)\n",
    "            cropped = crop_to_pupil(path, output_size=size)\n",
    "            save_path = os.path.join(output_folder, fname)\n",
    "            cv2.imwrite(save_path, cropped)\n",
    "\n",
    "# === 4. Augmentation (for training only) ===\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2)\n",
    "])\n",
    "\n",
    "def augment_image(img_pil, n_augmentations=2):\n",
    "    augmented = [augmentation_transforms(img_pil) for _ in range(n_augmentations)]\n",
    "    grayscale = transforms.Grayscale()(img_pil)\n",
    "    augmented.append(grayscale.convert(\"RGB\"))\n",
    "\n",
    "    np_img = np.array(img_pil)\n",
    "    gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    edge_pil = Image.fromarray(edge_rgb)\n",
    "    augmented.append(edge_pil)\n",
    "    return augmented\n",
    "\n",
    "# === 5. Build DataFrame for Training ===\n",
    "def build_dataframe(folder, label, preprocess, model, device='cuda', augment=True, n_aug=3):\n",
    "    df = pd.DataFrame(columns=range(512))\n",
    "    idx = 0\n",
    "    for image_path in tqdm(glob(f\"{folder}/*.png\")):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        images = [image] + (augment_image(image, n_augmentations=n_aug) if augment else [])\n",
    "\n",
    "        for img in images:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "                feat = model.encode_image(tensor)\n",
    "                feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "                df.loc[idx, list(range(512))] = feat.cpu().numpy()[0]\n",
    "                df.loc[idx, 'category'] = label\n",
    "                idx += 1\n",
    "    return df\n",
    "\n",
    "# === 6. Train Classifier ===\n",
    "def train_classifier(X, y):\n",
    "    model = LGBMClassifier(random_state=42)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# === 7. Evaluate Model ===\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    print(classification_report(y_val, y_pred))\n",
    "\n",
    "# === 8. Inference with Flags ===\n",
    "def infer_on_folder(folder_path, clf, preprocess, model, device='cuda',\n",
    "                    crop=True, strict_preprocess=True, batch_size=16):\n",
    "    results = []\n",
    "    image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path)\n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"🔍 Batched Inference\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            if crop:\n",
    "                img_cv = crop_to_pupil(path)\n",
    "                img = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "            if strict_preprocess:\n",
    "                img = preprocess(img)\n",
    "            else:\n",
    "                img = transforms.Resize((224, 224))(img)\n",
    "                img = transforms.ToTensor()(img)\n",
    "\n",
    "            images.append(img)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            batch = torch.stack(images).to(device)\n",
    "            embeddings = model.encode_image(batch)\n",
    "            embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "        preds = clf.predict(embeddings)\n",
    "        probs = clf.predict_proba(embeddings)\n",
    "\n",
    "        for j, path in enumerate(batch_paths):\n",
    "            results.append({\n",
    "                'image_path': path,\n",
    "                'prediction': 'cataract' if preds[j] == 1 else 'normal',\n",
    "                'prob_cataract': round(probs[j][1], 4),\n",
    "                'prob_normal': round(probs[j][0], 4),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === 9. Main Execution ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9738048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%| | 7/246 [00:00<00:28,  8.libpng warning: iCCP: known incorrect sRGB profile\n",
      " 15%|▏| 37/246 [00:15<00:46,  4libpng warning: iCCP: known incorrect sRGB profile\n",
      " 16%|▏| 40/246 [00:16<00:48,  4libpng warning: iCCP: known incorrect sRGB profile\n",
      " 22%|▏| 53/246 [00:24<01:12,  2libpng warning: iCCP: known incorrect sRGB profile\n",
      " 30%|▎| 74/246 [00:42<02:52,  1libpng warning: iCCP: known incorrect sRGB profile\n",
      " 75%|▊| 185/246 [01:00<00:04, 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_images/train/normal/image_34.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|▊| 192/246 [01:06<00:21,  libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      " 83%|▊| 205/246 [01:14<00:20,  libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      " 85%|▊| 209/246 [01:17<00:20,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 91%|▉| 225/246 [01:39<00:29,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 97%|▉| 238/246 [01:43<00:04,  libpng warning: iCCP: known incorrect sRGB profile\n",
      "100%|█| 246/246 [01:47<00:00,  \n",
      "  5%| | 12/245 [00:06<01:58,  1libpng warning: iCCP: known incorrect sRGB profile\n",
      " 10%| | 25/245 [00:10<00:34,  6libpng warning: iCCP: known incorrect sRGB profile\n",
      " 13%|▏| 32/245 [00:13<01:44,  2libpng warning: iCCP: known incorrect sRGB profile\n",
      " 24%|▏| 60/245 [00:25<01:26,  2libpng warning: iCCP: known incorrect sRGB profile\n",
      " 29%|▎| 71/245 [00:31<01:42,  1libpng warning: iCCP: known incorrect sRGB profile\n",
      " 29%|▎| 72/245 [00:31<01:40,  1libpng warning: iCCP: known incorrect sRGB profile\n",
      " 47%|▍| 116/245 [00:53<00:26,  libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      " 50%|▍| 122/245 [00:57<01:03,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 58%|▌| 142/245 [01:09<01:03,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 64%|▋| 156/245 [01:15<00:34,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 67%|▋| 164/245 [01:19<00:49,  libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      " 71%|▋| 173/245 [01:24<00:42,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 84%|▊| 206/245 [02:00<03:24,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 86%|▊| 211/245 [02:03<01:02,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 88%|▉| 216/245 [02:04<00:20,  libpng warning: iCCP: known incorrect sRGB profile\n",
      " 93%|▉| 229/245 [02:12<00:06,  libpng warning: iCCP: known incorrect sRGB profile\n",
      "100%|█| 245/245 [02:30<00:00,  \n",
      "100%|█| 246/246 [01:00<00:00,  \n",
      "100%|█| 245/245 [00:59<00:00,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[276  20]\n",
      " [ 15 279]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94       296\n",
      "         1.0       0.93      0.95      0.94       294\n",
      "\n",
      "    accuracy                           0.94       590\n",
      "   macro avg       0.94      0.94      0.94       590\n",
      "weighted avg       0.94      0.94      0.94       590\n",
      "\n",
      "Best model: LGBMClassifier(max_depth=7, n_estimators=200, verbose=-1)\n",
      "{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200}\n",
      "\n",
      "⏱️ Total training time (embedding + training): 147.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Batched Inference:  50%|▌| 1libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Batched Inference: 100%|█| 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     image_path prediction  prob_cataract  \\\n",
      "0  processed_images/test/cataract/image_279.png   cataract         0.9997   \n",
      "1  processed_images/test/cataract/image_251.png   cataract         0.9992   \n",
      "2  processed_images/test/cataract/image_292.png   cataract         0.9997   \n",
      "3  processed_images/test/cataract/image_286.png   cataract         0.9995   \n",
      "4  processed_images/test/cataract/image_287.png   cataract         0.9977   \n",
      "\n",
      "   prob_normal  \n",
      "0       0.0003  \n",
      "1       0.0008  \n",
      "2       0.0003  \n",
      "3       0.0005  \n",
      "4       0.0023  \n",
      "Predicted cataract count (strict): 56\n",
      "⏱️ Inference time with strict_preprocess=True: 40.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔍 Batched Inference:  50%|▌| 1libpng warning: iCCP: known incorrect sRGB profile\n",
      "🔍 Batched Inference: 100%|█| 2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     image_path prediction  prob_cataract  \\\n",
      "0  processed_images/test/cataract/image_279.png   cataract         0.8214   \n",
      "1  processed_images/test/cataract/image_251.png   cataract         0.9812   \n",
      "2  processed_images/test/cataract/image_292.png   cataract         0.9991   \n",
      "3  processed_images/test/cataract/image_286.png   cataract         0.9958   \n",
      "4  processed_images/test/cataract/image_287.png   cataract         0.9989   \n",
      "\n",
      "   prob_normal  \n",
      "0       0.1786  \n",
      "1       0.0188  \n",
      "2       0.0009  \n",
      "3       0.0042  \n",
      "4       0.0011  \n",
      "Predicted cataract count (fast): 54\n",
      "⏱️ Inference time with strict_preprocess=False: 39.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Load model and preprocessor\n",
    "model, preprocess, tokenizer, device = load_clip()\n",
    "\n",
    "# Step A: Preprocess raw eye images (optional if already done)\n",
    "preprocess_folder(\"processed_images/train/normal\", \"processed_aug_aug/train/normal\")\n",
    "preprocess_folder(\"processed_images/train/cataract\", \"processed_aug_aug/train/cataract\")\n",
    "\n",
    "# Step B: Build dataset with augmentation\n",
    "start_train_time = time.time()\n",
    "df_normal = build_dataframe(\"processed_aug_aug/train/normal\", 0, preprocess, model, device=device)\n",
    "df_cataract = build_dataframe(\"processed_aug_aug/train/cataract\", 1, preprocess, model, device=device)\n",
    "df = pd.concat([df_normal, df_cataract]).astype(float).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df.iloc[:, :512]\n",
    "y = df['category']\n",
    "\n",
    "# Step C: Train model\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "clf = train_classifier(X_train, y_train)\n",
    "evaluate_model(clf, X_val, y_val)\n",
    "\n",
    "# Step D: (Optional) Grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "grid = GridSearchCV(LGBMClassifier(verbose=-1), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best model:\", grid.best_estimator_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "end_train_time = time.time()\n",
    "print(f\"\\n⏱️ Total training time (embedding + training): {end_train_time - start_train_time:.2f} seconds\")\n",
    "\n",
    "# Step E: Inference on cataract test images WITH strict_preprocess\n",
    "start_infer_strict = time.time()\n",
    "df_results_cat_strict = infer_on_folder(\n",
    "    \"processed_images/test/cataract/\", clf, preprocess, model,\n",
    "    device=device, crop=True, strict_preprocess=True, batch_size=32\n",
    ")\n",
    "end_infer_strict = time.time()\n",
    "print(df_results_cat_strict.head())\n",
    "print(\"Predicted cataract count (strict):\", (df_results_cat_strict['prob_cataract'] > 0.5).sum())\n",
    "print(f\"⏱️ Inference time with strict_preprocess=True: {end_infer_strict - start_infer_strict:.2f} seconds\")\n",
    "\n",
    "# Step F: Inference on cataract test images WITHOUT strict_preprocess\n",
    "start_infer_fast = time.time()\n",
    "df_results_cat_fast = infer_on_folder(\n",
    "    \"processed_images/test/cataract/\", clf, preprocess, model,\n",
    "    device=device, crop=True, strict_preprocess=False, batch_size=32\n",
    ")\n",
    "end_infer_fast = time.time()\n",
    "print(df_results_cat_fast.head())\n",
    "print(\"Predicted cataract count (fast):\", (df_results_cat_fast['prob_cataract'] > 0.5).sum())\n",
    "print(f\"⏱️ Inference time with strict_preprocess=False: {end_infer_fast - start_infer_fast:.2f} seconds\")\n",
    "\n",
    "# (Optional) Repeat above block for normal images if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc91b7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Head'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(['Head',\"Tail\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3c3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
