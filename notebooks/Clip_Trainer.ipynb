{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226f752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alok.gupta/Desktop/MCP/mcp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "import albumentations as A\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "albumentations_aug = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=0.3),\n",
    "        A.GaussianBlur(blur_limit=5, p=0.5),\n",
    "        A.MedianBlur(blur_limit=5, p=0.5),\n",
    "    ], p=0.6),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Downscale(scale_min=0.7, scale_max=0.95, p=0.3),\n",
    "    A.RandomResizedCrop(size=(672, 672), scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.4),\n",
    "    A.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n",
    "    A.ImageCompression(quality_lower=30, quality_upper=80, compression_type='jpeg', p=0.2),\n",
    "])\n",
    "\n",
    "# === 1. Load CLIP Model ===\n",
    "def load_clip(device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "    model.to(device).eval()\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    return model, preprocess, tokenizer, device\n",
    "\n",
    "# === 2. Pupil Cropping ===\n",
    "def crop_to_pupil(image_path, output_size=(224, 224)):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_blur = cv2.medianBlur(gray, 5)\n",
    "\n",
    "    circles = cv2.HoughCircles(gray_blur, cv2.HOUGH_GRADIENT, dp=1.5, minDist=30,\n",
    "                                param1=50, param2=30, minRadius=20, maxRadius=150)\n",
    "\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        x, y, r = circles[0][0]\n",
    "        pad = int(r * 1.5)\n",
    "        x1, y1 = max(0, x - pad), max(0, y - pad)\n",
    "        x2, y2 = min(image.shape[1], x + pad), min(image.shape[0], y + pad)\n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "    else:\n",
    "        print(f\"⚠️ Pupil not detected in {image_path}, using full image.\")\n",
    "        cropped = image\n",
    "\n",
    "    resized = cv2.resize(cropped, output_size)\n",
    "    return resized\n",
    "\n",
    "# === 3. Save Cropped Images ===\n",
    "def preprocess_folder(input_folder, output_folder, size=(224, 224)):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for fname in tqdm(os.listdir(input_folder)):\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            path = os.path.join(input_folder, fname)\n",
    "            cropped = crop_to_pupil(path, output_size=size)\n",
    "            save_path = os.path.join(output_folder, fname)\n",
    "            cv2.imwrite(save_path, cropped)\n",
    "\n",
    "# === 4. Augmentation (for training only) ===\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2)\n",
    "])\n",
    "\n",
    "def augment_image(img_pil, n_augmentations=2):\n",
    "    augmented = [augmentation_transforms(img_pil) for _ in range(n_augmentations)]\n",
    "    grayscale = transforms.Grayscale()(img_pil)\n",
    "    augmented.append(grayscale.convert(\"RGB\"))\n",
    "\n",
    "    np_img = np.array(img_pil)\n",
    "    gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    edge_pil = Image.fromarray(edge_rgb)\n",
    "    augmented.append(edge_pil)\n",
    "    return augmented\n",
    "\n",
    "# === 5. Build DataFrame for Training ===\n",
    "def build_dataframe(folder, label, preprocess, model, device='cuda', augment=True, n_aug=3):\n",
    "    df = pd.DataFrame(columns=range(512))\n",
    "    idx = 0\n",
    "    for image_path in tqdm(glob(f\"{folder}/*.png\")):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        images = [image]\n",
    "        if augment:\n",
    "            np_img = np.array(image)\n",
    "            for _ in range(n_aug):\n",
    "                aug_img = albumentations_aug(image=np_img)['image']\n",
    "                aug_img_pil = Image.fromarray(aug_img)\n",
    "                images.append(aug_img_pil)\n",
    "            images += augment_image(image, n_augmentations=0)\n",
    "\n",
    "        for img in images:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "                feat = model.encode_image(tensor)\n",
    "                feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "                df.loc[idx, list(range(512))] = feat.cpu().numpy()[0]\n",
    "                df.loc[idx, 'category'] = label\n",
    "                idx += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97a100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▌                                                                                                     | 6/246 [00:01<01:08,  3.52it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      "  3%|██▉                                                                                                     | 7/246 [00:01<01:00,  3.95it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "with mlflow.start_run(run_name=\"CLIP_Pytotch_TL_Cataract_Classifier\"):\n",
    "\n",
    "    # Log environment info\n",
    "    mlflow.set_tag(\"clip_model\", \"ViT-B-32\")\n",
    "    mlflow.set_tag(\"pytorch\",\"ViT-B-32\")\n",
    "    mlflow.log_param(\"device\", 'cpu')\n",
    "    mlflow.log_param(\"n_augmentations\", 3)\n",
    "    model, preprocess, tokenizer, device = load_clip()\n",
    "\n",
    "    # Step A: Preprocess (optional)\n",
    "    preprocess_folder(\"processed_images/train/normal\", \"processed_aug_aug/train/normal\")\n",
    "    preprocess_folder(\"processed_images/train/cataract\", \"processed_aug_aug/train/cataract\")\n",
    "\n",
    "    # Step B: Build dataset\n",
    "    start_train_time = time.time()\n",
    "    df_normal = build_dataframe(\"processed_aug_aug/train/normal\", 0, preprocess, model, device=device)\n",
    "    df_cataract = build_dataframe(\"processed_aug_aug/train/cataract\", 1, preprocess, model, device=device)\n",
    "    df = pd.concat([df_normal, df_cataract]).astype(float).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db7ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 246/246 [01:59<00:00,  2.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 245/245 [01:08<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "df_normal = build_dataframe(\"processed_aug_aug/train/normal\", 0, preprocess, model, device=device)\n",
    "df_cataract = build_dataframe(\"processed_aug_aug/train/cataract\", 1, preprocess, model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86675b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "class CataractDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augment_fn=None, target_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        root_dir: path to 'processed_aug_aug/train'\n",
    "        transform: torchvision transforms to convert to tensor and normalize\n",
    "        augment_fn: albumentations pipeline (optional)\n",
    "        \"\"\"\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {'normal': 0, 'cataract': 1}\n",
    "\n",
    "        for label_name in ['normal', 'cataract']:\n",
    "            folder = os.path.join(root_dir, label_name)\n",
    "            paths = glob(f\"{folder}/*.png\") + glob(f\"{folder}/*.jpg\") + glob(f\"{folder}/*.jpeg\")\n",
    "            self.image_paths.extend(paths)\n",
    "            self.labels.extend([self.class_to_idx[label_name]] * len(paths))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.augment_fn = augment_fn\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Crop to pupil\n",
    "        image = crop_to_pupil(image_path, output_size=self.target_size)  # np.array in BGR\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply albumentations (optional)\n",
    "        if self.augment_fn:\n",
    "            image = self.augment_fn(image=image)['image']\n",
    "\n",
    "        # Convert to PIL for torchvision transforms\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99773604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# CLIP-normalization or standard ImageNet if using ResNet/Vit\n",
    "clip_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "clip_std  = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "# Final torchvision transform\n",
    "final_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=clip_mean, std=clip_std)\n",
    "])\n",
    "\n",
    "# Train dataset and loader\n",
    "train_dataset = CataractDataset(\n",
    "    root_dir='processed_aug_aug/train',\n",
    "    transform=final_transform,\n",
    "    augment_fn=albumentations_aug,\n",
    "    target_size=(224, 224)  # or 512x512 depending on model\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e2dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce35f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 1. Model Setup ===\n",
    "\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model.visual  # Only image encoder\n",
    "        embed_dim = clip_model.text_projection.shape[1]\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.clip(x)  # returns image embedding (B, 512)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def freeze_clip_layers(clip_visual, n_frozen_layers=10):\n",
    "    \"\"\"\n",
    "    Freezes first n_frozen_layers of the CLIP ViT encoder.\n",
    "    For ViT-B/32, total blocks = 12\n",
    "    \"\"\"\n",
    "    for name, param in clip_visual.named_parameters():\n",
    "        if 'transformer.resblocks' in name:\n",
    "            block_idx = int(name.split('.')[2])\n",
    "            if block_idx < n_frozen_layers:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        elif 'ln_post' in name or 'proj' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False  # patch embedding, pos_embed, etc.\n",
    "\n",
    "# === 2. Training Utilities ===\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# === 3. Full Training Function ===\n",
    "\n",
    "def run_training(train_loader, val_loader, n_frozen_layers=10, num_epochs=5, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load CLIP model\n",
    "    clip_model, preprocess, tokenizer = open_clip.create_model_and_transforms(\n",
    "        model_name='ViT-B-32',\n",
    "        pretrained='laion2b_s34b_b79k'\n",
    "    )\n",
    "    model = CLIPClassifier(clip_model).to(device)\n",
    "\n",
    "    # Freeze layers\n",
    "    freeze_clip_layers(model.clip, n_frozen_layers=n_frozen_layers)\n",
    "\n",
    "    # Define optimizer and loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    head_params = model.classifier.parameters()\n",
    "    clip_params = [p for p in model.clip.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': clip_params, 'lr': lr / 10},\n",
    "        {'params': head_params, 'lr': lr}\n",
    "    ])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} | Frozen CLIP layers: {n_frozen_layers}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a09c7d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5 | Frozen CLIP layers: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███████████████████████████████████▋                                                           | 6/16 [00:45<01:10,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  19%|█████████████████▍                                                                           | 3/16 [00:21<01:32,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6063 | Train Acc: 65.38%\n",
      "Val   Loss: 0.5089 | Val   Acc: 77.39%\n",
      "\n",
      "Epoch 2/5 | Frozen CLIP layers: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████████████████████████████████████████████████████████████████████████▍                 | 13/16 [01:21<00:16,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  50%|██████████████████████████████████████████████▌                                              | 8/16 [00:47<00:48,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4566 | Train Acc: 77.80%\n",
      "Val   Loss: 0.4007 | Val   Acc: 82.08%\n",
      "\n",
      "Epoch 3/5 | Frozen CLIP layers: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|████████████████████████████████████████████████████████████████▋                             | 11/16 [01:04<00:29,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  94%|██████████████████████████████████████████████████████████████████████████████████████▎     | 15/16 [01:24<00:05,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3892 | Train Acc: 80.24%\n",
      "Val   Loss: 0.3655 | Val   Acc: 82.69%\n",
      "\n",
      "Epoch 4/5 | Frozen CLIP layers: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|██████████████████████████████████████████████████████████████████████████████████▎           | 14/16 [01:27<00:13,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  38%|██████████████████████████████████▉                                                          | 6/16 [00:57<01:37,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3402 | Train Acc: 85.34%\n",
      "Val   Loss: 0.3220 | Val   Acc: 87.58%\n",
      "\n",
      "Epoch 5/5 | Frozen CLIP layers: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███████████████████████████████████▋                                                           | 6/16 [00:58<01:39,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  75%|█████████████████████████████████████████████████████████████████████                       | 12/16 [01:44<00:29,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Pupil not detected in processed_aug_aug/train/normal/image_83.png, using full image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2966 | Train Acc: 87.17%\n",
      "Val   Loss: 0.2862 | Val   Acc: 88.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (clip): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_training(train_loader,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1efc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
